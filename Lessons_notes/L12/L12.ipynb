{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torchsummary import summary\n",
    "import glob  # Import library for finding all files matching a pattern\n",
    "from PIL import Image  # Import library for image processing\n",
    "import numpy as np  # Import library for numerical operations (not used here)\n",
    "import os  # Import library for operating system functionalities\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import torch\n",
    "from datetime import datetime, timedelta  # Import libraries for date and time manipulation\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim  # Optimization algorithms for training the model\n",
    "import torch.nn.functional as F  # Common loss functions and activation functions\n",
    "from scipy.stats import spearmanr, pearsonr  # Statistical functions for correlation calculation\n",
    "import itertools  # Utility functions for generating combinations\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR  # Learning rate scheduler for training\n",
    "import matplotlib.pyplot as plt  # Plotting library for visualization\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the chosen device for training\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageOpenerDataset(Dataset):\n",
    "    def __init__(self, file_list, image_dir, gt_dir, transform=None):\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.image_paths = np.array([])\n",
    "        self.mask_paths = np.array([])\n",
    "        \n",
    "        with open(file_list, \"r\") as f:\n",
    "            self.image_paths = f.readlines()\n",
    "        \n",
    "        self.mask_paths = self.image_paths.copy()\n",
    "        \n",
    "        for i in range(len(self.image_paths)):\n",
    "            self.image_paths[i] = os.path.join(image_dir, self.image_paths[i].strip()) + \".jpg\"\n",
    "            self.mask_paths[i] = os.path.join(gt_dir, self.mask_paths[i].strip()) + \".png\"\n",
    "        \n",
    "        \n",
    "        self.image_paths.sort()\n",
    "        self.mask_paths.sort()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array(Image.open(self.image_paths[idx]))\n",
    "        mask = np.array(Image.open(self.mask_paths[idx]))\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        return  image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "path = '../datasets/VOC_exercise/'\n",
    "target_size = (256, 256)\n",
    "\n",
    "aug = A.Compose([\n",
    "    A.Resize(height=256, width=256, interpolation=cv2.INTER_NEAREST),\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "trainaug = A.Compose([\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Resize(height=256, width=256, interpolation=cv2.INTER_NEAREST),\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "trainset = ImageOpenerDataset(file_list=os.path.join(path, 'train.txt'),\n",
    "                             image_dir=os.path.join(path, 'images'),\n",
    "                             gt_dir=os.path.join(path, 'targets'),\n",
    "                             transform=trainaug)\n",
    "valset = ImageOpenerDataset(file_list=os.path.join(path, 'validation.txt'),\n",
    "                                image_dir=os.path.join(path, 'images'),\n",
    "                                gt_dir=os.path.join(path, 'targets'),\n",
    "                                transform=aug)\n",
    "\n",
    "testset = ImageOpenerDataset(file_list=os.path.join(path, 'test.txt'),\n",
    "                                image_dir=os.path.join(path, 'images'),\n",
    "                                gt_dir=os.path.join(path, 'targets'),\n",
    "                                transform=aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SegmentationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegmentationNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, k,ernel_size=3, padding=1)\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            )\n",
    "        self.upconv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.upconv1(x)\n",
    "        x = self.upconv2(x)\n",
    "        \n",
    "\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SegmentationNet().to(device)\n",
    "# Load data and train the model\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_iou(outputs, labels, num_classes):\n",
    "    \"\"\"\n",
    "    Calculates the Intersection over Union (IoU) for each class.\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): Model outputs of shape (batch_size, num_classes, height, width).\n",
    "        labels (torch.Tensor): Ground truth labels of shape (batch_size, height, width).\n",
    "        num_classes (int): Number of classes.\n",
    "\n",
    "    Returns:\n",
    "        iou (torch.Tensor): IoU for each class of shape (num_classes,).\n",
    "    \"\"\"\n",
    "    outputs = outputs.argmax(dim=1)  # Get the predicted class for each pixel\n",
    "    iou = torch.zeros(num_classes)\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        true_positives = ((outputs == cls) & (labels == cls)).sum().float()\n",
    "        false_positives = ((outputs == cls) & (labels != cls)).sum().float()\n",
    "        false_negatives = ((outputs != cls) & (labels == cls)).sum().float()\n",
    "        denominator = true_positives + false_positives + false_negatives\n",
    "\n",
    "        if denominator == 0:\n",
    "            iou[cls] = 0\n",
    "        else:\n",
    "            iou[cls] = true_positives / denominator\n",
    "\n",
    "    return iou\n",
    "\n",
    "def evaluate(net, dataloader, train, criterion, optimizer, scheduler):\n",
    "    try:\n",
    "        pbar.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    num_classes = 21\n",
    "\n",
    "    running_loss = []\n",
    "    running_iou = torch.zeros(num_classes)\n",
    "    correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "\n",
    "    pbar = tqdm(total=len(dataloader), desc=f\"{'Train' if train else 'Validation'}\", leave=True)\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        if train:\n",
    "            outputs = net(inputs)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                outputs = net(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss.append(loss.item())\n",
    "\n",
    "        iou = calculate_iou(outputs, labels, num_classes)\n",
    "        running_iou += iou\n",
    "\n",
    "        correct_pixels += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "        total_pixels += torch.numel(labels)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"{'Train' if train else 'Validation'} Loss: {np.mean(running_loss):.6f}, Acc: {correct_pixels / total_pixels:.4f}, mIoU: {running_iou.mean():.4f}\"\n",
    "        )\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    return correct_pixels / total_pixels, running_iou.mean()\n",
    "\n",
    "\n",
    "\n",
    "def train(net, trainloader, valloader, epochs, criterion, optimizer, scheduler, continue_training=''):\n",
    "    previous_epoch = 0  # Initialize previous_epoch to 0\n",
    "\n",
    "    try:\n",
    "        print(f\"Found best model, calculating acc...\")\n",
    "        \n",
    "        checkpoint = torch.load(os.path.join(path, 'net_best.pth'))\n",
    "        best_model = checkpoint['model']\n",
    "        best_criterion = checkpoint['loss']\n",
    "        best_scheduler = checkpoint['scheduler']\n",
    "        best_optimizer = checkpoint['optimizer']\n",
    "        best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_criterion.load_state_dict(checkpoint['criterion_state_dict'])\n",
    "        best_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        best_acc = evaluate(net=best_model, dataloader=valloader, train=False, criterion=best_criterion, optimizer=best_optimizer, scheduler=best_scheduler)\n",
    "        \n",
    "        print(f\"SROCC best model: {best_acc:.3f}\")\n",
    "        del best_model, best_optimizer, best_criterion, best_scheduler, checkpoint, gt_labels, pr_labels\n",
    "    except Exception as e:\n",
    "        best_acc = -1\n",
    "        print(e)\n",
    "        print(\"No best model found, starting from scratch\")\n",
    "\n",
    "    if continue_training != '':\n",
    "        try:\n",
    "            checkpoint = torch.load(os.join(path, f'net_{continue_training}.pth'))\n",
    "            net = checkpoint['model']\n",
    "            criterion = checkpoint['loss']\n",
    "            scheduler = checkpoint['scheduler']\n",
    "            optimizer = checkpoint['optimizer']\n",
    "            net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            criterion.load_state_dict(checkpoint['criterion_state_dict'])\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            previous_epoch = checkpoint['epoch']  # Update previous_epoch\n",
    "            epochs += previous_epoch  # Update total number of epochs to train\n",
    "            \n",
    "            del checkpoint\n",
    "            print(f\"Continuing training of {continue_training} model, checkpoint at epoch {previous_epoch}\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"No {continue_training} checkpoint found, starting from scratch\")\n",
    "\n",
    "    for epoch in range(previous_epoch, epochs):  # Loop over the dataset for multiple epochs\n",
    "        print(f\"Epoch {epoch}/{epochs}: \")\n",
    "        \n",
    "        net.train()  # Set model to training mode\n",
    "        evaluate(net=net,\n",
    "                dataloader=trainloader,\n",
    "                train=True,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler)\n",
    "        \n",
    "        net.eval()  # Set model to evaluation mode\n",
    "        acc = evaluate(net = net,\n",
    "                dataloader = valloader,\n",
    "                train=False,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler)\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'criterion_state_dict': criterion.state_dict(),\n",
    "                'model' : net,\n",
    "                'loss': criterion,\n",
    "                'optimizer': optimizer,\n",
    "                'scheduler': scheduler,\n",
    "                }, '../datasets/imdb/net_best.pth')\n",
    "            print(f\"New best model saved with SROCC: {sp:.3f}\")\n",
    "            \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'criterion_state_dict': criterion.state_dict(),\n",
    "            'model' : net,\n",
    "            'loss': criterion,\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "            }, f'../datasets/imdb/net_last.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "model = SegmentationNet().to(device)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=bs, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=bs, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found best model, calculating acc...\n",
      "[Errno 2] No such file or directory: '../datasets/VOC_exercise/net_best.pth'\n",
      "No best model found, starting from scratch\n",
      "Epoch 0/10: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [64, 32, 2, 2], expected input[64, 128, 256, 256] to have 64 channels, but got 128 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCosineAnnealingLR\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 129\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, trainloader, valloader, epochs, criterion, optimizer, scheduler, continue_training)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m net\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set model to evaluation mode\u001b[39;00m\n\u001b[1;32m    137\u001b[0m acc \u001b[38;5;241m=\u001b[39m evaluate(net \u001b[38;5;241m=\u001b[39m net,\n\u001b[1;32m    138\u001b[0m         dataloader \u001b[38;5;241m=\u001b[39m valloader,\n\u001b[1;32m    139\u001b[0m         train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    140\u001b[0m         criterion\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[1;32m    141\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m    142\u001b[0m         scheduler\u001b[38;5;241m=\u001b[39mscheduler)\n",
      "Cell \u001b[0;32mIn[5], line 50\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(net, dataloader, train, criterion, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     47\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[0;32m---> 50\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m, in \u001b[0;36mSegmentationNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupconv2(x)\n\u001b[1;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv5(x))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv6(x)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/conv.py:952\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    947\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    948\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    950\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [64, 32, 2, 2], expected input[64, 128, 256, 256] to have 64 channels, but got 128 channels instead"
     ]
    }
   ],
   "source": [
    "\n",
    "train(model, trainloader, valloader, 10, nn.CrossEntropyLoss(), optim.Adam(model.parameters(), lr=0.001), CosineAnnealingLR(optim.Adam(model.parameters(), lr=0.001), T_max=10), continue_training='best') # Train the model for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
